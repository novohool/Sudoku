{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get update\n",
    "!apt install nvidia-cuda-toolkit --yes\n",
    "!nvcc --version\n",
    "!pip install unsloth\n",
    "!pip install transformers==4.41.0\n",
    "!pip install torch==2.5.1\n",
    "!pip install peft==0.10.0\n",
    "!pip install datasets==2.16.1\n",
    "!pip install matplotlib==3.8.0 pandas==2.2.2 numpy==1.26 tqdm==4.66.2\n",
    "!pip install --upgrade accelerate bitsandbytes\n",
    "\n",
    "# 验证安装\n",
    "import unsloth\n",
    "import transformers\n",
    "import torch\n",
    "import peft\n",
    "import datasets\n",
    "print('All dependencies installed successfully.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from datetime import datetime\n",
    "import json\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import random\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import Dataset\n",
    "from unsloth import GRPOTrainer, GRPOConfig  # 假设 unsloth 提供了这些接口"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Download Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'Qwen/Qwen2.5-7B-Instruct'\n",
    "\n",
    "# 下载并配置 tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.save_pretrained('/content/qwen2.5-7b-tokenizer')\n",
    "print('Tokenizer downloaded and saved locally.')\n",
    "\n",
    "# 下载模型\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map='auto',\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model.save_pretrained('/content/qwen2.5-7b-model', safe_serialization=False)\n",
    "print('Model downloaded and saved locally.')\n",
    "\n",
    "# 应用 LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=16,  # LoRA rank\n",
    "    lora_alpha=32,\n",
    "    target_modules=['q_proj', 'v_proj'],\n",
    "    lora_dropout=0.05,\n",
    "    bias='none',\n",
    "    task_type='CAUSAL_LM'\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Configuration Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.max_seq_length = 3000  # 最大序列长度\n",
    "        self.batch_size = 1  # 批次大小\n",
    "        self.gradient_accumulation_steps = 8  # 梯度累积步数\n",
    "        self.learning_rate = 3e-4  # 学习率\n",
    "        self.max_steps = 500  # 最大步数\n",
    "        self.eval_every = 10  # 评估频率\n",
    "        self.dataset_path = '/content/sudoku.csv'  # 数据集路径\n",
    "        self.output_dir = f'/content/sudoku_rl_results_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}'\n",
    "        self.save_checkpoints = True\n",
    "        self.plot_metrics = True\n",
    "        self.log_interval = 10\n",
    "        self.lora_rank = 16  # LoRA rank\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# 设置日志\n",
    "os.makedirs(config.output_dir, exist_ok=True)\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[logging.FileHandler(os.path.join(config.output_dir, 'experiment_log.txt')), logging.StreamHandler()]\n",
    ")\n",
    "logging.info(f'Experiment started at {datetime.now()}')\n",
    "logging.info(f'Configuration: {vars(config)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_grid(grid_str):\n",
    "    \"\"\"格式化数独网格为可读形式\"\"\"\n",
    "    grid = [grid_str[i:i+9] for i in range(0, 81, 9)]\n",
    "    formatted = []\n",
    "    for i, row in enumerate(grid):\n",
    "        formatted.append(' | '.join(row))\n",
    "        if i % 3 == 2 and i < 8:\n",
    "            formatted.append('-' * 21)\n",
    "    return '\\n'.join(formatted)\n",
    "\n",
    "def extract_answer(response):\n",
    "    \"\"\"从模型响应中提取答案\"\"\"\n",
    "    match = re.search(r'<answer>(.*?)</answer>', response, re.DOTALL)\n",
    "    return match.group(1).strip() if match else ''\n",
    "\n",
    "def normalize_grid(grid_str):\n",
    "    \"\"\"规范化数独网格字符串，仅保留数字\"\"\"\n",
    "    return ''.join([c for c in grid_str if c.isdigit()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Load and Prepare Dataset (Optimized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sudoku_data(config):\n",
    "    \"\"\"加载数独数据集并分割为训练和评估集\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(config.dataset_path)\n",
    "        logging.info(f'Loaded {len(df)} Sudoku puzzles from {config.dataset_path}')\n",
    "    except FileNotFoundError:\n",
    "        logging.error(f'File {config.dataset_path} not found!')\n",
    "        return [], []\n",
    "    except Exception as e:\n",
    "        logging.error(f'Error loading Sudoku data: {e}')\n",
    "        return [], []\n",
    "\n",
    "    train_data = []\n",
    "    eval_data = []\n",
    "    total_size = len(df)\n",
    "    eval_size = int(total_size * 0.2)\n",
    "    train_size = total_size - eval_size\n",
    "\n",
    "    # 逐行处理数据，避免一次性加载所有数据到内存\n",
    "    for idx, row in df.iterrows():\n",
    "        puzzle_str = row['puzzle']\n",
    "        solution_str = row['solution']\n",
    "        formatted_puzzle = format_grid(puzzle_str)\n",
    "        prompt = f'Solve this 9x9 Sudoku puzzle:\\n{formatted_puzzle}\\nProvide step-by-step reasoning in <think> tags and the final grid in <answer> tags.'\n",
    "        clues = sum(1 for c in puzzle_str if c != '0')\n",
    "        difficulty = 1 if clues >= 50 else 2 if clues >= 40 else 3 if clues >= 30 else 4\n",
    "        data = {\n",
    "            'prompt': prompt,\n",
    "            'puzzle': puzzle_str,\n",
    "            'solution': solution_str,\n",
    "            'formatted_puzzle': formatted_puzzle,\n",
    "            'formatted_solution': format_grid(solution_str),\n",
    "            'difficulty': difficulty,\n",
    "        }\n",
    "        if idx < train_size:\n",
    "            train_data.append(data)\n",
    "        else:\n",
    "            eval_data.append(data)\n",
    "\n",
    "    logging.info(f'Split into {len(train_data)} training examples and {len(eval_data)} evaluation examples')\n",
    "    return train_data, eval_data\n",
    "\n",
    "train_data, eval_data = load_sudoku_data(config)\n",
    "train_dataset = Dataset.from_list(train_data)\n",
    "eval_dataset = Dataset.from_list(eval_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Define Reward Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tags_presence_reward_func(completions):\n",
    "    \"\"\"检查标签存在性奖励\"\"\"\n",
    "    rewards = []\n",
    "    for r in completions:\n",
    "        reward = 0.0\n",
    "        if '<think>' in r: reward += 0.25\n",
    "        if '</think>' in r: reward += 0.25\n",
    "        if '<answer>' in r: reward += 0.25\n",
    "        if '</answer>' in r: reward += 0.25\n",
    "        rewards.append(reward)\n",
    "    return rewards\n",
    "\n",
    "def tags_order_reward_func(completions):\n",
    "    \"\"\"检查标签顺序奖励\"\"\"\n",
    "    rewards = []\n",
    "    for r in completions:\n",
    "        think_start = r.find('<think>')\n",
    "        think_end = r.find('</think>')\n",
    "        answer_start = r.find('<answer>')\n",
    "        answer_end = r.find('</answer>')\n",
    "        if (think_start != -1 and think_end != -1 and\n",
    "            answer_start != -1 and answer_end != -1 and\n",
    "            think_start < think_end < answer_start < answer_end):\n",
    "            rewards.append(1.0)\n",
    "        else:\n",
    "            reward = 0.0\n",
    "            if think_start != -1 and think_end != -1 and think_start < think_end:\n",
    "                reward += 0.5\n",
    "            if answer_start != -1 and answer_end != -1 and answer_start < answer_end:\n",
    "                reward += 0.5\n",
    "            if think_end != -1 and answer_start != -1 and think_end < answer_start:\n",
    "                reward += 0.5\n",
    "            rewards.append(min(reward, 1.0))\n",
    "    return rewards\n",
    "\n",
    "def rule_compliance_reward_func(completions):\n",
    "    \"\"\"检查数独规则合规性奖励\"\"\"\n",
    "    rewards = []\n",
    "    for r in completions:\n",
    "        answer = extract_answer(r)\n",
    "        digits = normalize_grid(answer)\n",
    "        if len(digits) != 81:\n",
    "            rewards.append(0.0)\n",
    "            continue\n",
    "        try:\n",
    "            grid = np.array([int(d) for d in digits]).reshape(9, 9)\n",
    "            valid_rows = valid_columns = valid_boxes = 0\n",
    "            for i in range(9):\n",
    "                if len(set(grid[i, :])) == 9:\n",
    "                    valid_rows += 1\n",
    "            for j in range(9):\n",
    "                if len(set(grid[:, j])) == 9:\n",
    "                    valid_columns += 1\n",
    "            for box_i in range(3):\n",
    "                for box_j in range(3):\n",
    "                    box = grid[box_i*3:(box_i+1)*3, box_j*3:(box_j+1)*3].flatten()\n",
    "                    if len(set(box)) == 9:\n",
    "                        valid_boxes += 1\n",
    "            total_units = 27  # 9 rows + 9 columns + 9 boxes\n",
    "            valid_units = valid_rows + valid_columns + valid_boxes\n",
    "            rewards.append(valid_units / total_units)\n",
    "        except Exception as e:\n",
    "            logging.error(f'Error in rule compliance check: {e}')\n",
    "            rewards.append(0.0)\n",
    "    return rewards\n",
    "\n",
    "def enhanced_partial_answer_reward_func(completions, puzzle_str, solution_str):\n",
    "    \"\"\"增强的部分答案奖励\"\"\"\n",
    "    rewards = []\n",
    "    for r, puzzle, solution in zip(completions, puzzle_str, solution_str):\n",
    "        answer = extract_answer(r)\n",
    "        digits = normalize_grid(answer)\n",
    "        if len(digits) != 81:\n",
    "            rewards.append(0.0)\n",
    "            continue\n",
    "        for p, d in zip(puzzle, digits):\n",
    "            if p != '0' and p != d:\n",
    "                rewards.append(0.0)\n",
    "                break\n",
    "        else:\n",
    "            empty_cells = [i for i, c in enumerate(puzzle) if c == '0']\n",
    "            correct_empty = sum(1 for i in empty_cells if digits[i] == solution[i])\n",
    "            rewards.append(correct_empty * 0.25)\n",
    "    return rewards\n",
    "\n",
    "def combined_reward_function(completions, puzzle_str, solution_str):\n",
    "    \"\"\"综合奖励函数\"\"\"\n",
    "    reward_functions = [\n",
    "        (tags_presence_reward_func, 0.5),  # 格式合规性权重\n",
    "        (tags_order_reward_func, 0.5),\n",
    "        (rule_compliance_reward_func, 2.0),  # 网格结构权重\n",
    "        (enhanced_partial_answer_reward_func, 3.0),  # 准确性权重\n",
    "    ]\n",
    "    total_rewards = [0.0] * len(completions)\n",
    "    for reward_func, weight in reward_functions:\n",
    "        try:\n",
    "            if reward_func == enhanced_partial_answer_reward_func:\n",
    "                rewards = reward_func(completions, puzzle_str, solution_str)\n",
    "            else:\n",
    "                rewards = reward_func(completions)\n",
    "            for i, reward in enumerate(rewards):\n",
    "                total_rewards[i] += reward * weight\n",
    "        except Exception as e:\n",
    "            logging.error(f'Error in {reward_func.__name__}: {e}')\n",
    "    return total_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8: Training Loop with GRPO (Optimized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRPO 配置\n",
    "grpo_config = GRPOConfig(\n",
    "    model_name=model_name,\n",
    "    learning_rate=config.learning_rate,\n",
    "    batch_size=config.batch_size,\n",
    "    gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "    max_steps=config.max_steps,\n",
    "    ppo_epochs=1,\n",
    "    max_grad_norm=1.0\n",
    ")\n",
    "\n",
    "# 初始化 GRPO 训练器\n",
    "grpo_trainer = GRPOTrainer(\n",
    "    config=grpo_config,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=train_dataset\n",
    ")\n",
    "\n",
    "# 训练循环（分批加载数据）\n",
    "rewards_history = []\n",
    "for step in tqdm(range(config.max_steps), desc='Training'):\n",
    "    # 分批加载数据，避免一次性加载整个数据集\n",
    "    batch = random.sample(train_data, min(config.batch_size, len(train_data)))\n",
    "    prompts = [data['prompt'] for data in batch]\n",
    "    puzzle_str = [data['puzzle'] for data in batch]\n",
    "    solution_str = [data['solution'] for data in batch]\n",
    "    \n",
    "    # 生成模型输出\n",
    "    inputs = tokenizer(prompts, return_tensors='pt', truncation=True, max_length=config.max_seq_length).to('cuda')\n",
    "    response_ids = grpo_trainer.generate(inputs['input_ids'], max_length=config.max_seq_length)\n",
    "    completions = [tokenizer.decode(ids, skip_special_tokens=True) for ids in response_ids]\n",
    "    \n",
    "    # 计算奖励\n",
    "    rewards = combined_reward_function(completions, puzzle_str, solution_str)\n",
    "    rewards_tensor = torch.tensor(rewards, dtype=torch.float).to('cuda')\n",
    "    \n",
    "    # 更新模型\n",
    "    grpo_trainer.step(inputs['input_ids'], response_ids, rewards_tensor)\n",
    "    \n",
    "    # 日志和评估\n",
    "    avg_reward = np.mean(rewards)\n",
    "    rewards_history.append(avg_reward)\n",
    "    if (step + 1) % config.log_interval == 0:\n",
    "        logging.info(f'Step {step + 1}, Average Reward: {avg_reward}')\n",
    "    \n",
    "    if (step + 1) % config.eval_every == 0:\n",
    "        eval_rewards = []\n",
    "        eval_batch = random.sample(eval_data, min(10, len(eval_data)))\n",
    "        for data in eval_batch:\n",
    "            prompt = data['prompt']\n",
    "            inputs = tokenizer(prompt, return_tensors='pt', truncation=True, max_length=config.max_seq_length).to('cuda')\n",
    "            response_ids = model.generate(**inputs, max_length=config.max_seq_length)\n",
    "            response = tokenizer.decode(response_ids[0], skip_special_tokens=True)\n",
    "            reward = combined_reward_function([response], [data['puzzle']], [data['solution']])[0]\n",
    "            eval_rewards.append(reward)\n",
    "        logging.info(f'Step {step + 1}: Evaluation Reward: {np.mean(eval_rewards)}')\n",
    "        \n",
    "        if config.save_checkpoints:\n",
    "            checkpoint_dir = os.path.join(config.output_dir, f'checkpoint_step_{step + 1}')\n",
    "            model.save_pretrained(checkpoint_dir)\n",
    "            tokenizer.save_pretrained(checkpoint_dir)\n",
    "            logging.info(f'Saved checkpoint at {checkpoint_dir}')\n",
    "\n",
    "# 保存最终模型\n",
    "final_model_dir = os.path.join(config.output_dir, 'final_model')\n",
    "model.save_pretrained(final_model_dir)\n",
    "tokenizer.save_pretrained(final_model_dir)\n",
    "logging.info(f'Training completed. Final model saved to {final_model_dir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 9: Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.plot_metrics:\n",
    "    plt.plot(rewards_history)\n",
    "    plt.xlabel('Training Step')\n",
    "    plt.ylabel('Average Reward')\n",
    "    plt.title('Training Reward Over Time')\n",
    "    plt.savefig(os.path.join(config.output_dir, 'reward_curve.png'))\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
